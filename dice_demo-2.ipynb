{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feae676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "from flax.training import train_state, checkpoints\n",
    "import optax\n",
    "from flax import linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6f2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annulus_data(n_samples, n_t, key):\n",
    "    dt = 5e-2\n",
    "    def V(p):\n",
    "        x, y = p\n",
    "        r = (x-0.5)**2 + (y-0.5)**2\n",
    "        return (r - 0.1)**2\n",
    "\n",
    "\n",
    "    def u(p):\n",
    "        x, y = p\n",
    "        return -jax.grad(V)(p) + 0.5 * jnp.array([y - 0.5, -(x - 0.5)])\n",
    "\n",
    "\n",
    "    def time_step(x, key):\n",
    "        x += dt * jax.vmap(u)(x) + 0.05 * \\\n",
    "            jax.random.normal(key, x.shape) * jnp.sqrt(dt)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @jax.jit\n",
    "    def integrate(x, keys):\n",
    "        def step(x, key):\n",
    "            x = time_step(x, key)\n",
    "            return x, x\n",
    "        _, x_traj = jax.lax.scan(step, x, keys)\n",
    "        return x_traj\n",
    "    \n",
    "    x0 = jax.random.normal(key, (n_samples, 2)) * 0.1 + jnp.array([0.5, 0.5])\n",
    "    return integrate(x0, jax.random.split(key, n_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6c5bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Parameters\n",
    "###\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "d = 2           # spatial dimension\n",
    "Nt = 256        # number of time steps\n",
    "Nx = 10_000     # number of samples\n",
    "Nμ = 1          # number of parameter samples\n",
    "\n",
    "x_data = annulus_data(Nx, Nt, key)\n",
    "t_data = jnp.linspace(0, 1, Nt)\n",
    "μ_data = jnp.zeros(Nμ)\n",
    "\n",
    "nt = 256\n",
    "nx = 1024\n",
    "nμ = 1\n",
    "\n",
    "num_iterations = 20_000\n",
    "initial_learning_rate = 5e-4\n",
    "final_learning_rate   = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65a4516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  num_hid : int\n",
    "  num_out : int\n",
    "  num_layers : int\n",
    "\n",
    "  def setup(self):\n",
    "    self.layers = [nn.Dense(features=self.num_hid) for _ in range(self.num_layers)]\n",
    "    self.out = nn.Dense(features=self.num_out)\n",
    "\n",
    "  def __call__(self, x, t, mu):\n",
    "    h = jnp.hstack([x,t,mu])\n",
    "    for layer in self.layers:\n",
    "        h = nn.swish(layer(h))\n",
    "    h = self.out(h)\n",
    "    return h\n",
    "\n",
    "###\n",
    "# Neural network representing s = s(x, t, μ)\n",
    "###\n",
    "net_width = 64\n",
    "net_depth = 3\n",
    "model = MLP(num_hid=net_width, \n",
    "            num_out=1, \n",
    "            num_layers=net_depth) # NN representing s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80e52c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Initialize the model and optimizer\n",
    "###\n",
    "key, p_key = jax.random.split(key)\n",
    "params = model.init(p_key, jnp.zeros(d), jnp.zeros(1), jnp.zeros(1))\n",
    "learning_rate_schedule = optax.cosine_decay_schedule(\n",
    "    init_value=initial_learning_rate, \n",
    "    decay_steps=num_iterations,\n",
    "    alpha=final_learning_rate)\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "\n",
    "def s(params, x, t, mu):\n",
    "    return state.apply_fn(params, x, t, mu).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "901f2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility functions\n",
    "\n",
    "def get_random_subset(key, arr, bs):\n",
    "  N = len(arr)\n",
    "  indices = jax.random.choice(key, N, shape=(bs,), replace=False)\n",
    "  subset = arr[indices]\n",
    "  return subset\n",
    "\n",
    "def get_random_timegrid(key, t_data, bs_t):\n",
    "    n_t = t_data.shape[0]\n",
    "    t_q = get_random_subset(key, t_data, bs_t)\n",
    "    t_q = jnp.sort(t_q)\n",
    "    t_q = t_q.at[0].set(t_data[0])\n",
    "    t_q = t_q.at[-1].set(t_data[-1])\n",
    "    w_q = 0.5 * jnp.concatenate([jnp.array([t_q[1] - t_q[0]]), (t_q[2:] - t_q[:-2]), jnp.array([t_q[-1] - t_q[-2]])]) \n",
    "    return t_q, w_q\n",
    "        \n",
    "def get_expected_value_fct(x_data, t_data, mu_data, bs_n):\n",
    "    n_t = t_data.shape[0]\n",
    "    def expected_value(f, tau, t, i_mu, key):\n",
    "        #rho is evaluated at tau and s is evaluated at t\n",
    "        # get a random subset of x at time t1\n",
    "        i_t = jnp.int32(tau / (t_data[-1] - t_data[0]) * (n_t - 1))\n",
    "        x = get_random_subset(key, x_data[:, i_t, i_mu, :], bs_n)\n",
    "        mu = mu_data[i_mu]\n",
    "        # evaluate f at the time t at all x_tau and average\n",
    "        # signature of f should be (x, t, mu) -> scalar\n",
    "        return jnp.mean(jax.vmap(lambda _x: f(_x, t, mu))(x))\n",
    "    return expected_value\n",
    "\n",
    "def get_s_derivatives(s):\n",
    "    def grad_s(x, t, mu):\n",
    "        return jax.grad(s)(x, t, mu)\n",
    "    def grad_s_squared(x, t, mu):\n",
    "        return jnp.sum(grad_s(x, t, mu)**2)\n",
    "    def partial_t_s(x, t, mu):\n",
    "        return jax.grad(s,1)(x, t, mu)\n",
    "    def laplace_s(x,t,mu):\n",
    "        return jnp.trace(jax.hessian(s)(x, t, mu))\n",
    "    return grad_s, grad_s_squared, partial_t_s, laplace_s\n",
    "\n",
    "### DICE\n",
    "\n",
    "def get_dice_loss(_s, x_data, t_data, mu_data, bs_n, bs_t, bs_mu):\n",
    "    \n",
    "    n_t = t_data.shape[0]\n",
    "    n_mu = mu_data.shape[0]\n",
    "    expected_value = get_expected_value_fct(x_data, t_data, mu_data, bs_n)\n",
    "    \n",
    "    def dice_loss_mu(state, params, key, i_mu):\n",
    "        \n",
    "        mu = mu_data[i_mu]\n",
    "        # closures for s\n",
    "        s = lambda x, t, mu: _s(params, x, t, mu)\n",
    "        grad_s, grad_s_squared, partial_t_s, laplace_s = get_s_derivatives(s)\n",
    "\n",
    "        key, x_key, t_key = jax.random.split(key, 3)\n",
    "        x_keys = jax.random.split(x_key, bs_t)\n",
    "        t_q, w_q = get_random_timegrid(t_key, t_data, bs_t)\n",
    "        \n",
    "        E_s = lambda tau, t, key: expected_value(s, tau, t, i_mu, key)\n",
    "        E_s_v = jax.vmap(E_s)\n",
    "\n",
    "        sum_En_snplus1 =      jnp.sum(E_s_v(t_q[:-1], t_q[1:],  x_keys[:-1]))\n",
    "        sum_Enplus1_sn =      jnp.sum(E_s_v(t_q[1:],  t_q[:-1], x_keys[1:]))\n",
    "        sum_En_sn =           jnp.sum(E_s_v(t_q[:-1], t_q[:-1], x_keys[:-1]))\n",
    "        sum_Enplus1_snplus1 = jnp.sum(E_s_v(t_q[1:],  t_q[1:],  x_keys[1:]))\n",
    "        loss = (+ 0.5 * sum_En_snplus1 \n",
    "                - 0.5 * sum_Enplus1_sn \n",
    "                + 0.5 * sum_En_sn \n",
    "                - 0.5 * sum_Enplus1_snplus1)\n",
    "        \n",
    "        E_grad_s_squared = lambda tau, t, key: expected_value(grad_s_squared, tau, t, i_mu, key)\n",
    "        loss += 0.5 * jnp.sum( w_q * jax.vmap(E_grad_s_squared)(t_q, t_q, x_keys))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def dice_loss(state, params, key):\n",
    "        key, mu_key = jax.random.split(key)\n",
    "        i_mus = jax.random.choice(mu_key, n_mu, shape=(bs_mu,), replace=False)\n",
    "        loss = jnp.mean(jax.vmap(lambda i_mu: dice_loss_mu(state, params, key, i_mu))(i_mus))\n",
    "        return loss\n",
    "    \n",
    "    return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07171f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Our loss function expects a tensor of shape (Nx, Nt, Nμ, d). \n",
    "# We have no μ here, so we add a dummy dimension.\n",
    "\n",
    "x_data = jnp.reshape(x_data, (Nt, Nx, 1, d))\n",
    "x_data = jnp.transpose(x_data, (1, 0, 2, 3))  # Shape is now (Nx, Nt, Nμ, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5331e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = get_dice_loss(s, x_data, t_data, μ_data, nx, nt, nμ)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, key):\n",
    "  grad_fn = jax.value_and_grad(loss_fn, argnums=1)\n",
    "  loss, grads = grad_fn(state, state.params, key)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state, loss\n",
    "\n",
    "key, loc_key = jax.random.split(key)\n",
    "state, loss = train_step(state, loc_key)  # compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████████████████████████████████████▌                                                                                                                      | 6619/20000 [00:51<01:43, 129.17it/s, loss=-0.04482279]"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Optimization loop\n",
    "###\n",
    "\n",
    "from tqdm import tqdm\n",
    "loss_plot = [ ]\n",
    "states =    [ ]\n",
    "key, loop_key = jax.random.split(key)\n",
    "\n",
    "with tqdm(range(num_iterations)) as pbar:\n",
    "    for iter in pbar:\n",
    "        key, _ = jax.random.split(key)\n",
    "        state, loss = train_step(state, key)\n",
    "        loss_plot.append(loss)\n",
    "        if iter % 10 == 0:\n",
    "            states.append(state) # we save the state every 10 iterations for diagnostics\n",
    "        pbar.set_postfix({\"loss\": loss})\n",
    "loss_plot_np = np.array(loss_plot)\n",
    "s_opt_state = states[-1]\n",
    "\n",
    "###\n",
    "# Alternatively: Load pre-trained model\n",
    "# state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "# s_opt_state = state\n",
    "# s_opt_state = checkpoints.restore_checkpoint(ckpt_dir=\"...\", target=s_opt_state)\n",
    "### (Path has to be absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a725e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate samples with explicit euler\n",
    "def euler(x, f, t, dt):\n",
    "    return x + dt * f(x, t)\n",
    "\n",
    "def generate_sample_s(x0, v, times): \n",
    "    def step(carry, t_next):\n",
    "        x, t_prev = carry\n",
    "        dt = t_next - t_prev\n",
    "        x_new = euler(x, v, t_prev, dt)\n",
    "        new_carry = (x_new, t_next)\n",
    "        return new_carry, x_new\n",
    "\n",
    "    init = (x0, times[0])\n",
    "    carry, xs = jax.lax.scan(step, init, times[1:])\n",
    "    xs = jnp.vstack([x0[None, ...], xs])\n",
    "    return xs\n",
    "\n",
    "def grad_s(params, x, t, mu):\n",
    "    return jax.grad(s,1)(params, x, t, mu)\n",
    "\n",
    "def generate_batch(params, nx, times, mu, key):\n",
    "    key, genkey = jax.random.split(key)\n",
    "    x0 = jax.random.normal(key, (nx, 2)) * 0.1 + jnp.array([0.5, 0.5])\n",
    "    v = lambda x, t: grad_s(params, x, t, mu)\n",
    "    x = jax.vmap(lambda x0: generate_sample_s(x0, v, times))(x0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd71b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "key, eval_key = jax.random.split(key)\n",
    "\n",
    "\n",
    "x_gen = jax.vmap(lambda mu, key: generate_batch(s_opt_state.params, Nx, t_data, mu, key), out_axes=2)(μ_data, jax.random.split(key, len(μ_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b13ddaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gif(x_data, name):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Define the colors for the quadrants\n",
    "    COLOR_TOP_RIGHT = 'purple'\n",
    "    COLOR_BOTTOM_RIGHT = 'black'\n",
    "    COLOR_BOTTOM_LEFT = 'teal'\n",
    "    COLOR_TOP_LEFT = 'grey'\n",
    "\n",
    "    # Get the initial positions (x and y) from the first frame\n",
    "    initial_positions = x_data[:, 0, 0, :]  # Shape (Nx, d)\n",
    "    initial_x = initial_positions[:, 0]\n",
    "    initial_y = initial_positions[:, 1]\n",
    "\n",
    "    # Create boolean masks for each quadrant based on initial positions\n",
    "    mask_top_right = (initial_x >= 0.5) & (initial_y >= 0.5)\n",
    "    mask_bottom_right = (initial_x >= 0.5) & (initial_y < 0.5)\n",
    "    mask_bottom_left = (initial_x < 0.5) & (initial_y < 0.5)\n",
    "    mask_top_left = (initial_x < 0.5) & (initial_y >= 0.5)\n",
    "\n",
    "    # Create an array to hold the color for each particle\n",
    "    # Initialize with a default color (e.g., black)\n",
    "    particle_colors = np.full(Nx, COLOR_BOTTOM_RIGHT, dtype=object)\n",
    "\n",
    "    # Use the masks to set the colors for each quadrant\n",
    "    particle_colors[mask_top_right] = COLOR_TOP_RIGHT\n",
    "    particle_colors[mask_bottom_left] = COLOR_BOTTOM_LEFT\n",
    "    particle_colors[mask_top_left] = COLOR_TOP_LEFT\n",
    "\n",
    "    # Create the initial scatter plot object, passing the array of colors to `c`\n",
    "    scatter = ax.scatter(\n",
    "        initial_x, \n",
    "        initial_y, \n",
    "        c=particle_colors,\n",
    "        s=2\n",
    "    )\n",
    "\n",
    "    def update(frame_num):\n",
    "        \"\"\"Updates the scatter plot positions for a given frame.\"\"\"\n",
    "        new_positions = x_data[:, frame_num, 0, :]\n",
    "        scatter.set_offsets(new_positions)\n",
    "        return scatter,\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=Nt, interval=50, blit=True)\n",
    "\n",
    "    print(\"Saving animation...\")\n",
    "    anim.save(name, writer='pillow', dpi=150)\n",
    "    print(f\"Animation saved as {name}\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7053914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving animation...\n",
      "Animation saved as generated.gif\n"
     ]
    }
   ],
   "source": [
    "save_gif(x_data, 'true.gif')\n",
    "save_gif(x_gen, 'generated.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa375a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b07c7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
